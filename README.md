# Rhythm Fusion: Synchronizing Audio and Motion Features for Music-Driven Dance Generation

Welcome to the GitHub repository for **Rhythm Fusion**. This repository provides sample videos showcasing dance sequences generated by our model and other existing models, including **FACT**, **Bailando**, and **EDGE**. The purpose of these comparisons is to highlight the performance improvements achieved by our approach in music-driven dance generation.

## ðŸ“Š Model Performance Comparison

Below is a summary of key performance metrics comparing our model with other state-of-the-art models:

| Model          | PFC (â†“) | FID (â†“) | Beat Score (â†‘) |
|----------------|----------|----------|-----------------|
| FACT           | 2.25     | 23.11    | 0.22            |
| Bailando       | 1.75     | 24.82    | 0.23            |
| EDGE           | 1.53     | 23.08    | 0.27            |
| **Our Model**  | **0.98** | **22.89** | **0.29**       |

- **PFC**: Per-frame consistency (lower is better)  
- **FID**: FrÃ©chet Inception Distance (lower is better)  
- **Beat Score**: Alignment of generated dance with musical beats (higher is better)

Our model achieves improved performance across all key metrics, demonstrating better synchronization and more realistic dance generation.

## ðŸŽ¥ Qualitative Results

The same input music is provided to all models, and the output samples are collected accordingly. For example, files such as FACT_Sample1.mp4, Bailando_Sample1.mp4, EDGE_Sample1.mp4, OurModel_Sample1.mp4, and GroundTruth_Sample1.mp4 correspond to the same audio signal but with dance sequences generated by different models, as indicated by the file names. This structure is consistent across other samples, such as Sample2 and Sample3, allowing for direct comparisons between the models.

The videos provide side-by-side comparisons, making it easier to observe differences in synchronization and motion quality. In these comparisons, the FACT model displays some irregularities, such as jittering, particularly noticeable in Sample2. Both the FACT and Bailando models show less alignment with the musical beat, while EDGE and our model demonstrate better synchronization. Our model achieves superior alignment through the inclusion of two key components: the Fusion Sync Classifier and the Fusion Sync Enhancer, which improve the correspondence between audio and motion trajectories. (See the paper for more details about our architecture and these components.) Additionally, our model closely follows the beat and appears similar to the ground truth, further highlighting its improved synchronization performance.

